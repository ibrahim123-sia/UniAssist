"""
RAG QUERY SYSTEM FOR UNIVERSITY Q&A
"""

# ==================== SECTION 1: IMPORTS ====================
import chromadb
from sentence_transformers import SentenceTransformer
import json
import os
from dotenv import load_dotenv

# Load environment variables (for API keys)
load_dotenv()

# ==================== SECTION 2: INITIALIZE CHROMADB ====================
print("="*60)
print("INITIALIZING RAG QUERY SYSTEM")
print("="*60)

def init_chromadb():
    """Initialize ChromaDB connection"""
    print("üìÇ Loading ChromaDB vector database...")
    
    try:
        # Try modern API first
        client = chromadb.PersistentClient(path="./chroma_db")
        print("‚úÖ Using PersistentClient")
    except AttributeError:
        # Fall back to legacy API
        client = chromadb.Client(settings=chromadb.config.Settings(
            persist_directory="./chroma_db"
        ))
        print("‚úÖ Using Client with Settings")
    
    # Get the collection
    collection = client.get_collection(name="university_chunks")
    print(f"‚úÖ Loaded collection: 'university_chunks'")
    print(f"   Total documents: {collection.count()}")
    
    return collection

# ==================== SECTION 3: LOAD EMBEDDING MODEL ====================
def init_embedding_model():
    """Initialize the embedding model"""
    print("\nü§ñ Loading embedding model...")
    model = SentenceTransformer('all-MiniLM-L6-v2')
    print("‚úÖ Model loaded: all-MiniLM-L6-v2")
    return model

# ==================== SECTION 4: SEARCH FUNCTION ====================
def search_chromadb(question, collection, model, top_k=5):
    """
    Search ChromaDB for relevant chunks
    Returns: chunks, sources, and similarity scores
    """
    # Convert question to embedding
    question_embedding = model.encode(question).tolist()
    
    # Search ChromaDB
    results = collection.query(
        query_embeddings=[question_embedding],
        n_results=top_k,
        include=["documents", "metadatas", "distances"]
    )
    
    # Extract results
    chunks = results['documents'][0] if results['documents'] else []
    sources = results['metadatas'][0] if results['metadatas'] else []
    distances = results['distances'][0] if results['distances'] else []
    
    # Convert distances to similarity scores (higher = more similar)
    similarity_scores = [(1 - dist) for dist in distances] if distances else []
    
    return chunks, sources, similarity_scores

# ==================== SECTION 5: CREATE PROMPT ====================
def create_rag_prompt(question, relevant_chunks, sources):
    """
    Create a prompt for the LLM with context and instructions
    """
    # Combine chunks into context
    context_parts = []
    for chunk in relevant_chunks:
        context_parts.append(chunk)
    
    context = "\n\n---\n\n".join(context_parts)
    
    # Create the prompt - WITHOUT CITATION INSTRUCTIONS
    prompt = f"""You are a helpful assistant for Muhammad Ali Jinnah University (MAJU).
Your task is to answer student questions using ONLY the provided context from the university website.
If the context doesn't contain the answer, say "I don't have information about that in my database."

CONTEXT FROM UNIVERSITY WEBSITE:
{context}

STUDENT'S QUESTION: {question}

INSTRUCTIONS:
1. Answer clearly and concisely
2. Use ONLY information from the context above
3. DO NOT mention sources or citations in your answer
4. DO NOT include any numbers in brackets like [1] or [2]
5. DO NOT add a sources section at the end
6. Provide a clean, natural answer as if you're a university representative
7. If the context has contact info (emails, phones), include them naturally
8. If the context has fees or numbers, be precise

ANSWER:"""
    
    return prompt, sources

# ==================== SECTION 6: LLM INTEGRATION ====================
def get_llm_response(prompt, provider="groq"):
    """
    Get response from LLM (Groq)
    """
    if provider.lower() == "groq":
        return get_groq_response(prompt)
    else:
        return "[LLM NOT CONFIGURED - Install Groq]"

def get_groq_response(prompt):
    """Get response from Groq - UPDATED MODEL"""
    try:
        from groq import Groq
        
        api_key = os.getenv("GROQ_API_KEY")
        if not api_key:
            return "‚ùå GROQ_API_KEY not found in .env file"
        
        client = Groq(api_key=api_key)
        
        response = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {"role": "system", "content": "You are a helpful university assistant. Provide answers without citations or source references."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )
        
        return response.choices[0].message.content
        
    except ImportError:
        return "‚ùå Install Groq: pip install groq"
    except Exception as e:
        return f"‚ùå Groq Error: {str(e)}"

# ==================== SECTION 7: MAIN RAG FUNCTION ====================
def ask_university_rag(question, provider="groq"):
    """
    Main RAG pipeline: Question ‚Üí Search ‚Üí LLM ‚Üí Answer
    """
    print(f"\nüîç QUESTION: {question}")
    print("-"*60)
    
    # Initialize components
    collection = init_chromadb()
    model = init_embedding_model()
    
    # Step 1: Search for relevant chunks
    print(f"\nüìö Searching database for relevant information...")
    chunks, sources, scores = search_chromadb(question, collection, model, top_k=3)
    
    if not chunks:
        return "‚ùå No relevant information found in database."
    
    # Show what was found (for debugging only, not shown to user)
    print(f"‚úÖ Found {len(chunks)} relevant chunks (not shown to user)")
    
    # Step 2: Create prompt
    prompt, source_list = create_rag_prompt(question, chunks, sources)
    
    # Step 3: Get LLM response
    print(f"\nüí≠ Generating answer using {provider.upper()}...")
    answer = get_llm_response(prompt, provider)
    
    # Step 4: Clean the answer (remove any remaining citations)
    print(f"\nüìù FINALIZING CLEAN ANSWER...")
    
    # Remove any citation patterns that might have slipped through
    import re
    # Remove [1], [2], etc.
    answer = re.sub(r'\[\d+\]', '', answer)
    # Remove "Source:" or "Sources:" sections
    answer = re.sub(r'(?i)(sources?:.*?)(?=\n\n|\Z)', '', answer, flags=re.DOTALL)
    # Remove any trailing source lists
    answer = re.sub(r'(?i)\n\n(?:üìé\s*)?(?:source|references?):.*', '', answer, flags=re.DOTALL)
    
    # Return clean answer without sources
    return answer

# ==================== SECTION 8: BATCH TESTING ====================
def test_sample_questions():
    """Test the RAG system with sample student questions"""
    test_questions = [
        "What is the fee structure for BS Computer Science?",
        "How can I contact the admissions office?",
        "What scholarships are available?",
        "Tell me about the Computer Science faculty",
        "What are the admission requirements?"
    ]
    
    print("="*60)
    print("üß™ RUNNING SAMPLE TESTS")
    print("="*60)
    
    for i, question in enumerate(test_questions, 1):
        print(f"\nüß™ TEST {i}: {question}")
        answer = ask_university_rag(question, provider="groq")
        print(f"\nüì¢ ANSWER:\n{answer}")
        print("="*60)
        if i < len(test_questions):
            input("Press Enter for next test...")

# ==================== SECTION 9: INTERACTIVE MODE ====================
def interactive_mode():
    """Run interactive Q&A session"""
    print("="*60)
    print("üéØ UNIVERSITY RAG Q&A SYSTEM")
    print("="*60)
    print("Type 'quit' or 'exit' to end the session")
    print("Type 'test' to run sample questions")
    print("="*60)
    
    # Initialize once for the session
    print("\nüîÑ Initializing system...")
    collection = init_chromadb()
    model = init_embedding_model()
    print("‚úÖ System ready!")
    
    while True:
        print("\n" + "-"*40)
        question = input("\n‚ùì Student Question: ").strip()
        
        if question.lower() in ['quit', 'exit', 'q']:
            print("üëã Goodbye!")
            break
        
        if question.lower() == 'test':
            test_sample_questions()
            continue
        
        if not question:
            continue
        
        # Run RAG pipeline
        answer = ask_university_rag(question, provider="groq")
        print(f"\nüì¢ ANSWER:\n{answer}")

# ==================== SECTION 10: MAIN EXECUTION ====================
if __name__ == "__main__":
    import sys
    
    # Create .env file template if it doesn't exist
    if not os.path.exists(".env"):
        with open(".env", "w") as f:
            f.write("# API Keys for RAG System\n")
            f.write("# Get your API key from console.groq.com\n")
            f.write("GROQ_API_KEY=your_groq_api_key_here\n\n")
            f.write("# OR use OpenAI (optional)\n")
            f.write("# OPENAI_API_KEY=your_openai_api_key_here\n")
        print("üìù Created .env template file. Please add your API keys!")
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "test":
            test_sample_questions()
        elif sys.argv[1] == "ask" and len(sys.argv) > 2:
            question = " ".join(sys.argv[2:])
            answer = ask_university_rag(question, provider="groq")
            print(f"\nüì¢ FINAL ANSWER:\n{answer}")
        else:
            print("Usage:")
            print("  python rag_query.py                     # Interactive mode")
            print("  python rag_query.py test                # Run sample tests")
            print("  python rag_query.py ask 'your question' # Ask single question")
    else:
        interactive_mode()